---
title: "Shiny data prep"
author: "Nick McManus"
date: "2023-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)    ## always
library(here)         ## consistent file paths
library(zoo)          ## interpolation of water data
library(stringr)      ## split up file names
library(lubridate)    ## mutate dates
library(terra)        ## better/faster GIS package
library(raster)       ## format plays nicer w/leaflet
library(sf)           ## format plays nicer w/leaflet
library(leaflet)      ## maps
library(htmlwidgets)  ## better map
library(htmltools)    ## better map
library(webshot2)     ## capture leaflet maps as .png
library(av)           ## convert .png to .mp4
```

This markdown is for wrangling and prepping all the data data that goes into the Shiny app. This includes some code chunks of test plots/maps that will be removed in the future. 

# Zip codes

We want to find and save the zip codes located within the Central Valley portion of Kern county. This .shp will then be exported for use in the Shiny app.

*Note*: For the purposes of this app, the extent of zip codes will be cropped to only portions within both Kern county and the Central Valley. To remove artifacts of zip code portions that only slightly intersect with Kern, we'll also remove any zip code with an area below 1,000,000 m^2. To keep the entire geometry of zips that are within Kern, see the commented out code chunk at the end of this section. 
```{r}
## Read in data w/ sf::st_read() bc sf obj easier to filter
## Then transform to SpatVect obj with terra::vect() for easier geoprocessing

## Read in all counties in CA, keep only Kern
kern <- st_read(here('data/counties_ca/cnty19_1.shp')) %>% 
  ## Only keep kern county
  dplyr::filter(COUNTY_NAM == "Kern") %>%
  dplyr::select(COUNTY_NAM) %>% 
  terra::vect() 

## Read in central valley shp
valley <- st_read(here('data/central_valley/Alluvial_Bnd.shp')) %>% 
  ## same crs as kern vect
  st_transform(crs = st_crs(kern)) %>% 
  vect() %>% 
  ## Only keep portion w/in Kern county
  terra::intersect(kern)

## Read in all zipcodes in CA
zips <- st_read(here('data/zipcodes/CA_Zips.shp')) %>%
  ## match kern crs
  st_transform(crs = st_crs(kern)) %>% 
  vect() %>% 
  ## only keep parts in kern & central valley
  terra::intersect(valley) %>% 
  st_as_sf() %>% 
  dplyr::select(GEOID10) %>% 
  rename(zipcode = GEOID10) %>% 
  ## change to better crs for calc local area
  st_transform(crs = "epsg:3310")

## find area and remove small cropped zips
zips_filter <- zips %>% 
  mutate(area_m2 = as.numeric(st_area(zips))) %>% 
  filter(area_m2 >= 1000000) %>% 
  vect()

## ensure CRS is leaflet friendly
kern_zips <- terra::project(zips_filter, "+proj=longlat +datum=WGS84")

## Save clipped zips
writeVector(kern_zips, here('data/zipcodes/kern_zips.shp'), overwrite = T)

## Save Kern and Valley vect for leaflet too
kern <- terra::project(kern, "+proj=longlat +datum=WGS84")
writeVector(kern, here('data/counties_ca/kern.shp'), overwrite = T)

valley <- terra::project(valley, "+proj=longlat +datum=WGS84")
writeVector(valley, here('data/central_valley/valley.shp'), overwrite = T)

```



# Standing water

### Convert to .tif
First we need to deal with format of some 2022 Landsat rasters. These are unmarked files types (no file extension) paired with an .hdr file. The unmarked file can only be read with the .hdr file present of the same name. This will make future steps of reading in layers tricky, so first we'll convert all these files to .tif format and remove the original file pairings. 
```{r}
convert_tif = function(path) {
  ## List the non .hdr files (can't be read in)
  list <- grep(list.files(path), pattern = ".hdr", invert = TRUE, value = TRUE)
  
  ## Read in and save as .tif
  for (i in 1:length(list)) {
    r = rast(paste0(path, list[i]))
    writeRaster(r, paste0(path,list[i],".tif"), overwrite = T)
  }
  
  ## List and remove original non .tif files
  nontifs <- grep(list.files(path, full.names = TRUE), 
                  pattern = ".tif", invert = TRUE, value = TRUE)
  file.remove(nontifs)
}

convert_tif(path = here("data/water/Landsat_Dan/newfiles_temp/42r36//"))

```

### Merge images
Our AOI is split between two Landsat images (rows 35 and row 36 within path 42). First we'll mask the rasters with the "QA_PIXEL" files to ensure only locations with water have a value of 1. Then, we'll merge the two row images by date, reproject it the crs for Leaflet, and crop/mask it Kern county. 
- **NOTE:** Change input/output file paths to 2022 or 2023 depending on year.
```{r}
## kern county
kern <- vect(here("data/counties_ca/kern.shp"))

## Create paths for water rasters and masks
path35 <- here('data/water/Landsat_Dan/p042r035/2023//')
path36 <- here('data/water/Landsat_Dan/p042r036/2023//')

## Fxn to extract dates based on LANDSAT naming convention
dates <- function(files) {
  str_split(files, "_", simplify = TRUE) %>% 
  as.data.frame() %>%
  mutate(date = lubridate::ymd(V4)) %>%
  dplyr::select(date)
}

## df with all files and dates
waterInput_df <- data.frame("rast35" = list.files(path35, pattern="UnmixedMask85"),
                            "mask35" = list.files(path35, pattern = "QA_PIXEL"),
                            "rast36" = list.files(path36, pattern="UnmixedMask85"),
                            "mask36" = list.files(path36, pattern="QA_PIXEL")) %>%
  ## extract date from r35
  mutate(dates(rast35),
         date = as.character(date))

## Fxn to mask, merge, then export for each date
waterMerge <- function(rast35, mask35, rast36, mask36, date, kern, pathOut) {
  ## raster and mask for row 35 -------------  
  r35 <- rast(paste0(path35, rast35))
  names(r35) <- "rast"
  m35 <- rast(paste0(path35, mask35))
  ## mask values of 1 are NA
  m35[m35 == 1] <- NA
  ## mask raster 35
  r_masked35 <- terra::mask(r35, m35)
 
  ## raster and mask for row 36 ---------------
  r36 <- rast(paste0(path36, rast36))
  names(r36) <- "rast"
  m36 <- rast(paste0(path36, mask36))
  ## mask values of 1 are NA
  m36[m36 == 1] <- NA
  ## mask raster 36
  r_masked36 <- terra::mask(r36, m36)
 
  ## merge rasts using SpatRastCollection -----
  s <- sprc(r_masked35, r_masked36)
  m <- merge(s)
  
  ## Save merged rasts (entire extent)
  terra::writeRaster(m,
                     filename = paste0(pathOut, "p42_merged_", date, ".tif"),
                     overwrite = TRUE)
  
  ## Crop merged rasts to Kern county ---------
  ## faster to reproject kern vect to raster crs
  kern_reproj = project(kern, y = crs(m))
  ## crop/mask merged rast to kern county
  m_kern = crop(m, kern_reproj, mask = TRUE)
  ## Save kern cropped/masked raster 
  terra::writeRaster(
    m_kern,
    filename = paste0(pathOut, "p42_kern_", date, ".tif"),
    overwrite = TRUE
  )
}

## Run fxn for all files in directory
purrr::pmap(
  .l = waterInput_df,
  .f = waterMerge,
  kern = kern,
  pathOut = here("data/water/p042_masked_merged/2023//"),
  .progress = TRUE
)
```


### Zonal stats
Here we determine how much standing water is present within each zip code for each image date. The `waterZonal()` fxn (a wrapper fxn with several more inside) reads in our rasters and vector of interest, reprojects where needed, and iterates the zonal function over all the rasters in a given directory. We'll run this fxn for 2023 and 2022 rasters, then merge the results and export a single .csv.
```{r}
waterZonal <- function(rastPath, vect) {
    ## Fxn to get dates from file names
    dates <- function(files) {
      str_split(files, "_", simplify = TRUE) %>% 
      as.data.frame() %>%
      mutate(date = lubridate::ymd(V3)) %>%
      dplyr::select(date)
    }
    
    ## One dataframe with all files and dates
    df <- data.frame("rasts" = list.files(rastPath, 
                                          ## we want the cropped/masked rasts
                                          pattern = "kern")) %>% 
      ## extract date from r35
      mutate(dates(rasts),
             rasts = paste0(rastPath, rasts))
    
    ## Need same crs for zonal stats.
    ## Easier to transform vector than the high-res raster
    ## Read in sample raster from list
    r <- rast(df$rasts[1])
    vect <- project(vect, y = crs(r))
    
    ## Fxn to perform zonal stats and output results
    zonalFxn <- function(rasts, vect, date) {
      r <- terra::rast(rasts)
      ## find total #cells with water by zip
      zonalStat <- terra::zonal(r, vect, fun = 'sum', na.rm = TRUE)
      ## store results in tidy df
      zonalStat_df <- zonalStat %>%
        ## assign values with corresponding zipcode
        mutate(zipcode = vect$zipcode, .before = 1) %>%
        ## Convert values from #cells to acres.
        ## At 30m resolution, each pixel is 900 m^2.
        ## 4046.86 m^2 are in one acre
        rename(ncells = "rast") %>%
        mutate(acres = ncells * 900 / 4046.86) %>%
        ## finally, add image date
        mutate(date = date)
    }
    
    ## Run fxn over list w/pmap
    waterStats_df <- pmap_dfr(.x = df, 
                              .f = zonalFxn, 
                              vect = vect,
                              .progress = TRUE)
    
} ##END FXN

## Paths for water rasters and zipcode vector
path23 <- here('data/water/p042_masked_merged/2023//')
path22 <- here('data/water/p042_masked_merged/2022//')
vect <- vect(here("data/zipcodes/kern_zips.shp"))

## Run fxn for 2023 and 2022 and bind results to one df
waterStats_df = rbind(waterZonal(rastPath = path22, vect = vect),
                      waterZonal(rastPath = path23, vect = vect)) %>%
  mutate(date_plot = format(as.Date(date), '%b-%d-%Y'))

## export as .csv
write_csv(waterStats_df, here('data/water/water_acre_zipcode.csv'))
```

##### Cloud issues
There are some cloud issues with masking for standing water. The values for select dates will be replaced by an averaged value from the date prior and following.
```{r}
## Read in data generated from pervious code chunk
waterStats_df = read_csv(here("data/water/water_acre_zipcode.csv")) %>% 
  mutate(date = as.character(date))

## Df interpolated & removed values
water_int_df = waterStats_df %>% 
  group_by(zipcode) %>% 
  ## remove first/last bad dates (not enough info to interpolate)
  filter(!date %in% c("2022-03-10", "2023-11-08", "2023-11-16")) %>% 
  ## For four dates, just avg values directly before/after
  mutate(acres_int = case_when(date %in% c("2022-06-06",
                                           "2022-11-13",
                                           "2023-04-22",
                                           "2023-06-09",
                                           "2023-07-03",
                                           "2023-09-21") 
                               ~((lead(acres, n = 1)+lag(acres, n = 1))/2),
                               ## These dates have larger gaps on either side, 
                               ## replace w/NA then interpolate
                               date %in% c("2023-03-13",
                                           "2023-05-08",
                                           "2023-05-16") 
                               ~NA,
                               .default = acres)) %>% 
  mutate(acres_int = zoo::na.approx(acres_int)) %>% 
  ## Repeat process for number of cells
  mutate(ncells_int = case_when(date %in% c("2022-06-06",
                                            "2022-11-13",
                                            "2023-04-22",
                                            "2023-06-09",
                                            "2023-07-03",
                                            "2023-09-21") 
                                ~(round(((lead(ncells, n = 1)+lag(ncells, n = 1))/2),0)),
                               date %in% c("2023-03-13",
                                           "2023-05-08",
                                           "2023-05-16") 
                               ~NA,
                               .default = ncells)) %>%   
  ## want whole num for #cells
  mutate(ncells_int = round(zoo::na.approx(ncells_int),0))

## save over as updated csv
write_csv(water_int_df, here("data/water/water_acre_zipcode.csv"))

```


### Water persistence raster
Here we'll perform some simple "raster math" to get a better idea of how frequently certain areas of Kern County are flooded. Because the extent of each satellite image is slightly different, we'll first have to ensure all rasters are the exact same extent by resampling.
```{r}
## List all rasters
path23 = here("data/water/p042_masked_merged/2023//")
path22 = here("data/water/p042_masked_merged/2022//")

rasts <- list.files(path22, pattern = "kern", full.names = FALSE) %>%
  append(., list.files(path23, pattern = "kern", full.names = FALSE))

## Fxn to extract date from file name
## Only works if "full.names" arg is FALSE when making list
dates <- function(files) {
      str_split(files, "_", simplify = TRUE) %>% 
      as.data.frame() %>%
      mutate(date = lubridate::ymd(V3)) %>%
      dplyr::select(date)
}

## Filter out bad dates from raster list
rasts_df <- data.frame(rasts = rasts,
                       date = dates(rasts)) %>%
  ## list all files in directory
  mutate(rasts = list.files(here("data/water/p042_masked_merged/"),
                                pattern="kern", recursive = TRUE, 
                                full.names=TRUE)) %>% 
  ## remove bad dates
  dplyr::filter(!date %in% c("2022-03-10","2022-06-06","2022-11-13",
                             "2023-03-13","2023-04-22","2023-05-08",
                             "2023-05-16","2023-06-09","2023-07-03", 
                             "2023-09-21","2023-11-08","2023-11-16"))

## Make all rasters same extent ---------------
## sample raster 
sample_r <- rast(rasts_df$rasts[1])

## "stack" resampled rasters in terra-friendly format
## resample rasters as being read into stack
rasts_list <- terra::rast(lapply(rasts_df$rasts, function(x) {
  r <- rast(x) %>%
    resample(y = sample_r,
             ##categorical (water/no water)
             method = "near")
}))

## Add stack layers, then replace 0 with NA
rasts_sum <- sum(rasts_list)
rasts_sum_na <- terra::classify(rasts_sum, cbind(0, NA))

## Save 30m version
writeRaster(rasts_sum_na, here("data/water/summed_water_30m_2022_2023.tif"))


## Save 90m version (faster load time in shiny)
r <- rast(here("data/water/summed_water_30m_2022_2023.tif"))
r_agg <- terra::aggregate(r, fact = 3, fun = "modal")
writeRaster(r_agg, here("data/water/summed_water_90m_2022_2023.tif"))
```


### Animate rasters
Here we'll generate a movie (.mp4) that goes through a time series of standing water images by zip code. This is done creating a series of maps using `leaflet`, capturing those maps as images (.png) using `webshot`, and finally converting the images into an .mp4. 

- **NOTE:** This was originally done as a .gif, but the file sizes were far to big to host them all on the Shiny. The code for converting .png to .gif (rather than .mp4) in commented out at the end of the fxn for future reference. 
```{r}
## List all rasters
path23 = here("data/water/p042_masked_merged/2023//")
path22 = here("data/water/p042_masked_merged/2022//")

rasts <- list.files(path22, pattern = "kern", full.names=FALSE) %>% 
  append(., list.files(path23, pattern="kern", full.names=FALSE))

## Fxn to extract date from file name
dates <- function(files) {
      str_split(files, "_", simplify = TRUE) %>% 
      as.data.frame() %>%
      mutate(date = lubridate::ymd(V3)) %>%
      dplyr::select(date)
 }

## list of all zipcodes
zips_sf <- read_sf(here("data/zipcodes/kern_zips.shp"))
zips_list = unique(zips_sf$zipcode)

## df of file paths and dates of rasters to be visualized
rasts_df = data.frame(rasts = rep(rasts, length(zips_list)),
                      zipcode = rep(zips_list, each=length(rasts))) %>% 
  ## Add dates, then remove select bad rasters (cloud)
  mutate(dates(rasts),
         date = as.character(date),
         rasts = rep(list.files(here("data/water/p042_masked_merged/"),
                                pattern="kern", recursive = TRUE, 
                                full.names=TRUE),
                     length(zips_list))) %>% 
  filter(!date %in% c("2022-03-10","2022-06-06","2022-11-13",
                      "2023-03-13","2023-04-22","2023-05-08",
                      "2023-05-16","2023-06-09","2023-07-03", 
                      "2023-09-21","2023-11-08","2023-11-16")) %>% 
  mutate(date_title = lubridate::ymd(date),
         date_title = format(date_title, "%m-%d-%Y"),
         date = gsub("-", "", date)) 


### Loop to create .mp4 for each zip code
for (j in 1:length(zips_list)) {
  ## Only look at rasters for one zip at a time
  rasts_filter <- rasts_df %>% 
    filter(zipcode == zips_list[j])
  
  ## Shows progress in console for user
  print(paste0("Working on zipcode: ", zips_list[j]))
  
      ## Save leaflet map as .png for each date  
      for(i in 1:nrow(rasts_filter)) {
        ## Read in each raster and zipcode as vect
        r <- rast(rasts_filter$rasts[i])
        values(r)[values(r) == 0] <- NA
  
        zip <- zips_sf %>%
          filter(zipcode == rasts_filter$zipcode[i]) %>%
          vect() %>%
          project(y=crs(r))
  
        ## crop/mask raster to zipcode
        r_crop <- r %>%
          crop(x=., y=zip, mask = TRUE)
        r_crop <- raster(r_crop)
        
        ## Limit map bounds to zipcode
        geom <- zips_sf %>%
          filter(zipcode == rasts_filter$zipcode[i])
        bounds <- geom %>%
          st_bbox() %>%
          as.character()
        
        ## Style for date box on map
        tag.map.title <- tags$style(HTML("
          .leaflet-control.map-title { 
            transform: translate(-50%,20%);
            position: fixed !important;
            left: 50%;
            text-align: center;
            padding-left: 10px; 
            padding-right: 10px; 
            border-style: solid;
            border-width: 1.5px;
            border-color: #222021;
            background: rgba(255,255,255,0.6);
            font-weight: bold;
            font-size: 25px;
          }"))
          title <- tags$div(
            tag.map.title, rasts_filter$date_title[i]
          )  
  
          ## Create leaflet map
          m <-
            leaflet(options = leafletOptions(zoomControl = FALSE)) %>%
            ## world imagery base map
            addProviderTiles(providers$Esri.WorldImagery) %>%
            ## panes/layers on map; zip outline on bottom, water rast on top
            addMapPane("zips", zIndex = 410) %>%
            addMapPane("water", zIndex = 415) %>%
            ## add water raster to map
            addRasterImage(
              r_crop,
              colors = "blue",
              project = TRUE,
              options = pathOptions(pane = "water")
            ) %>%
            ## add zipcode to map
            addPolylines(
              stroke = TRUE,
              weight = 2,
              color = "black",
              fill = TRUE,
              fillColor = "white",
              fillOpacity = 0.5,
              data = geom,
              options = pathOptions(pane = "zips")
            ) %>%
            ## boundaries of map limited to zipcode 
            fitBounds(
              lng1 = bounds[1],
              lat1 = bounds[2],
              lng2 = bounds[3],
              lat2 = bounds[4]
            ) %>%
            ## add "title" with date to map
            addControl(title, position = "topright", className = "map-title")
          
        ## save map as .png
        htmlwidgets::saveWidget(m, 
                                ## only need html temporarily
                                ## once screenshot captured, can overwrite
                                here("data/water/temp/temp.html"), 
                                selfcontained = TRUE)
        
        webshot2::webshot(
          url = here("data/water/temp/temp.html"),
          ## save file name
          file = paste0(
            here("data/water/zips//"),
            sprintf("zip_%02s_", rasts_filter$zipcode[i]),
            sprintf("%02s.png", rasts_filter$date[i])
          ),
          cliprect = "viewport",
          ## give time for map to "load" before capturing
          delay = 1,
          zoom = 4
        )
      }##END inner loop
   
   ## List all pics of zip code
   png_files <- list.files(here("data/water/zips//"),
                          pattern = zips_list[j],
                          full.names = TRUE)
   ## Output .mp4 from images
   av::av_encode_video(
     png_files,
     framerate = 2,
     output = paste0(
       here("data/water/vids//"),
       "zip_",
       zips_list[j],
       "_2022_2023.mp4"
     )
   )
   
   
  #### Output .gif from images instead of .mp4 ----------
  # library(magick)
  # gif_convert <- function(x, output) {
  # image_read(x) %>% 
  #   image_animate(fps=1) %>% 
  #     ## try out "optimize" and fps arguments
  #     ## in this for future run
  #   image_write(output)
  # }
  # 
  # gif_convert(x= paste0(here("data/water/zips//"), png_files),
  #             output = paste0(here("data/water/gifs//"),
  #                             "zip_", zips_list[j], "_2022.gif"))
  
} ##END outer loop

```



# Temperature

The mean daily temperature by zip code was extracted from the PRISM dataset using a Google Earth Engine script. This was exported as a CSV with variables for date ("imageID"), zip code ("GEOID10"), and daily mean temperate ("mean"). We'll read in and clean up the CSV, as well as determine if each observation falls w/in the optimal range for WNV transmission. For *Culex tarsalis*, this is between 22.9-25.9C (optimum temp of 23.9 w/95% CI); for *Culex quinquefasciatus*, this is between 23.9-27.1C (optimum 25.2 w/95% CI). Becuse both species are present in Kern, we'll consider the optimal range from 22.9-27.1C (Shocket et al., 2020)
```{r}
## Read in GEE output
temp <- read_csv(here('data/temp/kern_tmean_GEE_output.csv'))

## Tidy up and add info on temp ranges
temp_tidy <- temp %>% 
  ## extract date from PRISM image id
  mutate(date = lubridate::ymd(imageID)) %>% 
  dplyr::select(!imageID) %>% 
  ## rename GEE extract variables
  rename(tmean_c = mean,
         zipcode = GEOID10) %>% 
  ## find temp in F
  mutate(tmean_f = (tmean_c*(9/5))+32,
         .before = date) %>% 
  ## does a day fall w/in optimal range for culex?
  mutate(cx_opt = case_when(tmean_c >= 12.1 & tmean_c < 22.9 ~ "in range",
                            tmean_c >= 22.9 & tmean_c <= 27.1 ~ "optimal",
                            tmean_c > 27.1 & tmean_c <=31.9 ~ "in range",
                            .default = "out range"))

write_csv(temp_tidy, here('data/temp/kern_tmean_20100401_20230930.csv'))
```




# Trap data

Finally, we'll wrangle some trap data and aggregate it by zip code. Exact locations of traps should not be public, so instead we'll produce plots showing how abundance and MIR values across a zip code change by month/year.

Because trap data is aggregated and assigned to clusters, we'll first need to assign each cluster to a zip code. 
```{r}
## shp file of clusters
clust <- read_sf(here("data/traps/andy/cluster_shp/clusterPolys.shp"))

## shp file of zips
zips <- read_sf(here("data/zipcodes/kern_zips.shp")) %>% 
  st_transform(crs = crs(clust))

## Assign a cluster to a zip code
clust_zips <- st_centroid(clust) %>% 
  st_join(zips) %>% 
  ## centroid of clust 7 and 95 *just* outside 93308
  ## manually adding to zips
  within(., zipcode[clust == 7] <- 93280) %>% 
  within(., area_m2[clust == 7] <- (zips$area_m2[zips$zipcode==93280])) %>% 
  within(., zipcode[clust == 95] <- 93308) %>% 
  within(., area_m2[clust == 95] <- zips$area_m2[zips$zipcode==93308])
```

Now we can assign zip codes to abundance and MIR data:
```{r}
## WNV MIR
wnv <- read_csv(here("data/traps/andy/wnvMIRPIR1500LagWeeks0_NA.csv")) %>%
  janitor::clean_names() %>%
  ## assign zip by clust
  inner_join(x = ., y = clust_zips) %>%
  ## filter for relevant vars
  dplyr::select(zipcode,
                clust:woy,
                date,
                pool_size,
                num_pools,
                mir_all,
                mir_spline_all) %>%
  mutate(month = lubridate::month(date),
         .before = year)

write_csv(wnv, here("data/traps/plotting/wnvMIR_plotting.csv"))

## SLEV MIR
slev <- read_csv(here("data/traps/andy/slevMIRPIR1500LagWeeks0_NA.csv")) %>%
  janitor::clean_names() %>%
  inner_join(x = ., y = clust_zips) %>%
  dplyr::select(zipcode,
                clust:woy,
                date,
                pool_size,
                num_pools,
                mir_all,
                mir_spline_all) %>%
  mutate(month = lubridate::month(date),
         .before = year)

write_csv(slev, here("data/traps/plotting/slevMIR_plotting.csv"))

### Abundance
abund <- read_csv(here('data/traps/andy/all1500LagWeeks0_NA.csv')) %>%
  janitor::clean_names() %>%
  ## assign zip by clust
  inner_join(x = ., y = clust_zips) %>%
  ## filter for relevant vars
  dplyr::select(zipcode, 
                clust:woy,
                collection_date, 
                mos_per_trap_night) %>%
  rename(date = collection_date) %>%
  mutate(month = lubridate::month(date),
         .before = year)
write_csv(abund, here("data/traps/plotting/abundance_plotting.csv"))
```



# R0

Once the model is complete, this chunk may be useful for finding average R0 by zip code. As a proof of concept, this was done using a sample transmission efficiency raster in Kern.
```{r}
## Read in data
wnv_trans <- rast(here('data/Kern_transmission_raster_wgs84.tif'))

## easier to transform vector than high-res raster
kern_zips <- vect(here("data/zipcodes/kern_zips.shp")) %>% 
  project(y = crs(wnv_trans))

## find average across all of Kern
trans_kern <- global(wnv_trans, fun = "mean", na.rm = TRUE)
trans_kern <- data.frame("trans_eff" = trans_kern[1,1], "zipcode" = "Kern")

## find the mean value per zip
trans_zonal <- terra::zonal(wnv_trans, kern_zips, fun = 'mean', na.rm = TRUE)

trans_zonal_zips <- trans_zonal %>% 
  mutate(zipcode = kern_zips$zipcode) %>% 
  rename(trans_eff = Kern_transmission_raster_wgs84) %>% 
  rbind(., trans_kern)

write_csv(trans_zonal_zips, here('data/transmission_efficiency_zipcodes.csv'))
```
