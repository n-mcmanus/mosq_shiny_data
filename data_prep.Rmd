---
title: "test"
author: "Nick McManus"
date: "2023-07-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)    ## always
library(zoo)          ## interpolation
library(here)         ## consistent file paths
library(stringr)      ## split up file names
library(lubridate)    ## mutate dates
library(terra)        ## better/faster GIS package
library(sf)           ## format plays nicer w/leaflet
library(raster)       ## format plays nicer w/leaflet
library(leaflet)      ## test interactive maps
```

This markdown is for wrangling and prepping data that goes into the Shiny app.


## Zip codes

We want to find and save the zip codes located within the Central Valley portion of Kern county. This .shp will then be exported for use in the Shiny app.

*Note*: For the purposes of this app, the extent of zip codes will be cropped to only portions within both Kern county and the Central Valley. To remove artifacts of zip code portions that only slightly intersect with Kern, we'll also remove any zip code with an area below 1,000,000 m^2. To keep the entire geometry of zips that are within Kern, see the commented out code chunk at the end of this section. 
```{r}
## Read in data w/ 'sf' pkg (easier to filter by attribute).
## After filtering, make SpatVect obj (`terra` pkg)

kern <- st_read(here('data/counties_ca/cnty19_1.shp')) %>% 
  ## Only keep kern county
  dplyr::filter(COUNTY_NAM == "Kern") %>%
  dplyr::select(COUNTY_NAM) %>% 
  terra::vect() 

valley <- st_read(here('data/central_valley/Alluvial_Bnd.shp')) %>% 
  st_transform(crs = st_crs(kern)) %>% 
  vect() %>% 
  ## Only keep portion w/in Kern
  terra::intersect(kern)

zips <- st_read(here('data/zipcodes/CA_Zips.shp')) %>% 
  st_transform(crs = st_crs(kern)) %>% 
  vect() %>% 
  terra::intersect(valley) %>% 
  st_as_sf() %>% 
  dplyr::select(GEOID10) %>% 
  rename(zipcode = GEOID10) %>% 
  ## best crs for calculating area(?)
  st_transform(crs = "epsg:3310")

## find area and remove small cropped zips
zips_filter <- zips %>% 
  mutate(area_m2 = as.numeric(st_area(zips))) %>% 
  filter(area_m2 >= 1000000) %>% 
  vect()

## ensure CRS is leaflet friendly
kern_zips <- terra::project(zips_filter, "+proj=longlat +datum=WGS84")

## Save clipped zips
writeVector(kern_zips, here('data/zipcodes/kern_zips.shp'), overwrite = T)

## Save Kern and Valley vect for leaflet too
kern <- terra::project(kern, "+proj=longlat +datum=WGS84")
writeVector(kern, here('data/counties_ca/kern.shp'), overwrite = T)

valley <- terra::project(valley, "+proj=longlat +datum=WGS84")
writeVector(valley, here('data/central_valley/valley.shp'), overwrite = T)

```

```{r}
# ## Read in raw data
# zips <- st_read(here('data/zipcodes/CA_Zips.shp'))
# counties <- st_read(here('data/counties_ca/cnty19_1.shp')) 
# 
# ## make sure both layers have same crs
# zips_trans <- zips %>% 
#   st_transform(zips, crs = st_crs(counties)) %>%
#   ## only need to keep zipcode number
#   dplyr::select(GEOID10) 
# 
# ## Only keep name of counties
# counties <- dplyr::select(counties, COUNTY_NAM)
# 
# # kern <- dplyr::filter(counties, COUNTY_NAM == "Kern") 
# 
# 
# ## Keep zipcodes that touch Kern county
# kern_zips <- st_join(zips_trans, counties, join = st_intersects) %>% 
#   dplyr::filter(COUNTY_NAM == "Kern", 
#                 ## remove select zips that barely touch Kern
#                 !GEOID10 %in% c(93536, 93535, 93201, 
#                                 93239, 93204, 93453,
#                                 93257, 93260, 93219, 
#                                 93261, 93218)) 
#   
# ## Reproject zips to leaflet-required crs
# kern_zips_trans <- st_transform(kern_zips, crs = "+proj=longlat +datum=WGS84")
# 
# ## save
# st_write(kern_zips_trans, here("data/zipcodes/kern_zips.shp"))

```


## Standing water

### Convert to .tif
First we need to deal with format of some 2022 Landsat rasters. These are unmarked files types (no file extension) paired with an .hdr file. The unmarked file can only be read with the .hdr file present of the same name. This will make future steps of reading in layers tricky, so first we'll convert all these files to .tif format and remove the original file pairings. 
```{r}
convert_tif = function(path) {
  ## List the non .hdr files (can't be read in)
  list <- grep(list.files(path), pattern = ".hdr", invert = TRUE, value = TRUE)
  
  ## Read in and save as .tif
  for (i in 1:length(list)) {
    r = rast(paste0(path, list[i]))
    writeRaster(r, paste0(path,list[i],".tif"), overwrite = T)
  }
  
  ## List and remove original non .tif files
  nontifs <- grep(list.files(path, full.names = TRUE), 
                  pattern = ".tif", invert = TRUE, value = TRUE)
  file.remove(nontifs)
}

convert_tif(path = here("data/water/Landsat_Dan/p042r035/2022//"))

```

### Merge images
Our AOI is split between two Landsat images (rows 35 and row 36 within path 42). First we'll mask the rasters with the "QA_PIXEL" files to ensure only locations with water have a value of 1. Then, we'll merge the two row images them by date, reproject it the crs for Leaflet, and crop/mask it Kern county. 
- **NOTE:** Change input/output file paths to 2022 or 2023 depending on year.
```{r}
## kern county
kern <- vect(here("data/counties_ca/kern.shp"))

## Create paths for water rasters and masks
path35 <- here('data/water/Landsat_Dan/p042r035/2023//')
path36 <- here('data/water/Landsat_Dan/p042r036/2023//')

## Fxn to extract dates based on LANDSAT naming convention
dates <- function(files) {
  str_split(files, "_", simplify = TRUE) %>% 
  as.data.frame() %>%
  mutate(date = lubridate::ymd(V4)) %>%
  dplyr::select(date)
}

## df with all files and dates
waterInput_df <- data.frame("rast35" = list.files(path35, pattern="UnmixedMask85"),
                            "mask35" = list.files(path35, pattern = "QA_PIXEL"),
                            "rast36" = list.files(path36, pattern="UnmixedMask85"),
                            "mask36" = list.files(path36, pattern="QA_PIXEL")) %>%
  ## extract date from r35
  mutate(dates(rast35),
         date = as.character(date))

## Fxn to mask, merge, then export for each date
waterMerge <- function(rast35, mask35, rast36, mask36, date, kern, pathOut) {
  ## raster and mask for row 35 -------------  
  r35 <- rast(paste0(path35, rast35))
  names(r35) <- "rast"
  m35 <- rast(paste0(path35, mask35))
  ## mask values of 1 are NA
  m35[m35 == 1] <- NA
  ## mask raster 35
  r_masked35 <- terra::mask(r35, m35)
 
  ## raster and mask for row 36 ---------------
  r36 <- rast(paste0(path36, rast36))
  names(r36) <- "rast"
  m36 <- rast(paste0(path36, mask36))
  ## mask values of 1 are NA
  m36[m36 == 1] <- NA
  ## mask raster 36
  r_masked36 <- terra::mask(r36, m36)
 
  ## merge rasts using SpatRastCollection
  s <- sprc(r_masked35, r_masked36)
  m <- merge(s)
  ## Save merged rasts
  terra::writeRaster(m, filename=paste0(pathOut,"p42_merged_",date,".tif"),
                     overwrite = TRUE)

  ## reproject kern vect to crs of raster bc faster
  kern_reproj = project(kern, y = crs(m))
  ## crop/mask merged rast to kern county
  m_kern = crop(m, kern_reproj, mask = TRUE)
  ## Save kern masked raster
  terra::writeRaster(m_kern, filename=paste0(pathOut,"p42_kern_",date,".tif"),
                     overwrite = TRUE)
}

## Run fxn for all files in directory
purrr::pmap(waterInput_df, waterMerge, kern=kern,
            pathOut = here("data/water/p042_masked_merged/2023//"),
            .progress = TRUE)
```


### Zonal stats
Here we determine how much standing water is present within each zip code for each image date. The larger `waterZonal()` fxn (which contains several more fxns inside) reads in our rasters and vector of interest, reprojects where needed, and iterates the zonal function over all the rasters in a given directory. We'll run this fxn for 2023 and 2022 rasters, then merge the results and export a single .csv.
```{r}
waterZonal <- function(rastPath, vect) {
    ## Fxn to get dates
    dates <- function(files) {
      str_split(files, "_", simplify = TRUE) %>% 
      as.data.frame() %>%
      mutate(date = lubridate::ymd(V3)) %>%
      dplyr::select(date)
    }
    
    ## One dataframe with all files and dates
    df <- data.frame("rasts" = list.files(rastPath, pattern="kern")) %>% 
      ## extract date from r35
      mutate(dates(rasts),
             rasts = paste0(rastPath, rasts))
    
    ## Need same crs for zonal stats.
    ## Easier to transform vector than the high-res raster
    r <- rast(df$rasts[1])
    vect <- project(vect, y = crs(r))
    
    ## Fxn to perform zonal stats and output results
    zonalFxn <- function(rasts, vect, date) {
      r = terra::rast(rasts)
      zonalStat <- terra::zonal(r, vect, fun = 'sum', na.rm = TRUE)
      zonalStat_df <- zonalStat %>%
        ## assign values with corresponding zipcode
        mutate(zipcode = vect$zipcode, .before = 1) %>%
        ## convert values from # of pixels to acres
        ## w/30m resolution, each pixel is 900 m^2
        ## 4046.86 m^2 are in one acre
        rename(ncells = "rast") %>%
        mutate(acres = ncells * 900 / 4046.86) %>%
        ## finally, add date image was taken
        mutate(date = date)
    }
    
    ## Run fxn over list w/pmap
    waterStats_df <- pmap_dfr(df, zonalFxn, vect=vect, 
                              .progress = TRUE)
    
} ##END FXN

## Paths for water rasters and zipcode vector
path23 <- here('data/water/p042_masked_merged/2023//')
path22 <- here('data/water/p042_masked_merged/2022//')
vect <- vect(here("data/zipcodes/kern_zips.shp"))

## Run fxn for 2023 and 2022 and bind results to one df
waterStats_df = rbind(waterZonal(rastPath=path22, vect=vect),
                      waterZonal(rastPath=path23, vect=vect))

## export as .csv
write_csv(waterStats_df, here('data/water/water_acre_zipcode.csv'))
```

##### Cloud issues
There are some cloud issues with masking for standing water. The values for select dates will be replaced by an averaged value from the date prior and following.
```{r}
waterStats_df = read_csv(here("data/water/water_acre_zipcode.csv")) %>% 
  mutate(date = as.character(date))

water_int_df = waterStats_df %>% 
  group_by(zipcode) %>% 
  ## remove first bad date complete 
  ## (not enough info to interpolate)
  filter(date != "2022-03-10") %>%
  ## For four dates, just avg values directly before/after
  mutate(acres_int = case_when(date %in% c("2022-06-06",
                                           "2022-11-13",
                                           "2023-04-22",
                                           "2023-06-09",
                                           "2023-07-03") 
                               ~((lead(acres, n = 1)+lag(acres, n = 1))/2),
                               ## B/c these dates have larger gaps on either side, 
                               ## replace w/NA then interpolate
                               date %in% c("2023-03-13",
                                           "2023-05-08",
                                           "2023-05-16") 
                               ~NA,
                               .default = acres)) %>% 
  mutate(acres_int = na.approx(acres_int)) %>% 
  mutate(ncells_int = case_when(date %in% c("2022-06-06",
                                            "2022-11-13",
                                            "2023-04-22",
                                            "2023-06-09",
                                            "2023-07-03") 
                                ~(round(((lead(ncells, n = 1)+lag(ncells, n = 1))/2),0)),
                               date %in% c("2023-03-13",
                                           "2023-05-08",
                                           "2023-05-16") 
                               ~NA,
                               .default = ncells)) %>%   
  mutate(ncells_int = round(na.approx(ncells_int),0))

write_csv(water_int_df, here("data/water/water_acre_zipcode.csv"))

```


### Animate rasters
Here we'll generate a movie (.mp4) that goes through a time series of standing water images by zip code. This is done creating a series of maps using `leaflet`, capturing those maps as images (.png), and finally converting the images into an .mp4. 

- **NOTE:** This was originally done as a .gif, but the file sizes were far to big to host them all on the Shiny. The code for converting .png to .gif (rather than .mp4) in commented out at the end of the fxn for reference. 
```{r}
## List all rasters
path23 = here("data/water/p042_masked_merged/2023//")
path22 = here("data/water/p042_masked_merged/2022//")

rasts <- list.files(path22, pattern = "kern", full.names=FALSE) %>% 
  append(., list.files(path23, pattern="kern", full.names=FALSE))

## Fxn to extract date from file name
dates <- function(files) {
      str_split(files, "_", simplify = TRUE) %>% 
      as.data.frame() %>%
      mutate(date = lubridate::ymd(V3)) %>%
      dplyr::select(date)
 }

## df of all zipcodes
zips_sf <- read_sf(here("data/zipcodes/kern_zips.shp"))
zips_list = unique(zips_sf$zipcode)

## df of file paths and dates of rasters to be visualized
rasts_df = data.frame(rasts = rep(rasts, length(zips_list)),
                      zipcode = rep(zips_list, each=length(rasts))) %>% 
  ## Add dates, then remove select rasters based on them
  mutate(dates(rasts),
         date = as.character(date),
         rasts = rep(list.files(here("data/water/p042_masked_merged/"),
                                pattern="kern", recursive = TRUE, 
                                full.names=TRUE),
                     length(zips_list))) %>% 
  filter(!date %in% c("2022-03-10","2022-06-06","2022-11-13",
                      "2022-03-13","2023-04-22","2023-05-08",
                      "2023-05-16","2023-06-09","2023-07-03")) %>% 
  mutate(date_title = lubridate::ymd(date),
         date_title = format(date_title, "%m-%d-%Y"),
         date = gsub("-", "", date)) 



library(webshot2)
# library(htmlwidgets)
# library(htmltools)
library(leaflet)
# library(magick)
library(av)
  
### Loop to create .mp4 for each zip code
for (j in 1:length(zips_list)) {
  ## Only look at rasters for one zip at a time
  rasts_filter <- rasts_df %>% 
    filter(zipcode == zips_list[j])
  
  ## Shows progress in console for user
  print(paste0("Working on zipcode: ", zips_list[j]))
  
      ## Save leaflet map as .png for each date  
      for(i in 1:nrow(rasts_filter)) {
        ## Read in each raster and zipcode as vect
        r <- rast(rasts_filter$rasts[i])
        values(r)[values(r) == 0] <- NA
  
        zip <- read_sf(here("data/zipcodes/kern_zips.shp")) %>%
          filter(zipcode == rasts_filter$zipcode[i]) %>%
          vect() %>%
          project(y=crs(r))
  
        ## crop/mask raster to zipcode
        r_crop <- r %>%
          crop(x=., y=zip, mask = TRUE)
        r_crop <- raster(r_crop)
        
        ## Limit map bounds to zipcode
        geom <- zips_sf %>%
          filter(zipcode == rasts_filter$zipcode[i])
        bounds <- geom %>%
          st_bbox() %>%
          as.character()
        
        ## Style for date box on map
        tag.map.title <- tags$style(HTML("
          .leaflet-control.map-title { 
            transform: translate(-50%,20%);
            position: fixed !important;
            left: 50%;
            text-align: center;
            padding-left: 10px; 
            padding-right: 10px; 
            border-style: solid;
            border-width: 1.5px;
            border-color: #222021;
            background: rgba(255,255,255,0.6);
            font-weight: bold;
            font-size: 15px;
          }"))
          title <- tags$div(
            tag.map.title, rasts_filter$date_title[i]
          )  
  
        ## add raster to map
        m <- leaflet(options = leafletOptions(zoomControl = FALSE)) %>%
          addProviderTiles(providers$Esri.WorldImagery) %>%
          ##zips outline bottom layer
          addMapPane("zips", zIndex = 410) %>%
          addMapPane("water", zIndex = 415) %>%
          addRasterImage(r_crop, colors = "blue", 
                         project = TRUE, options=pathOptions(pane="water")) %>%
          addPolylines(stroke=TRUE, weight = 2, color="black",
                     fill = TRUE, fillColor = "white", fillOpacity = 0.5,
                     data = geom, options=pathOptions(pane="zips")) %>%
          fitBounds(lng1 = bounds[1], lat1 = bounds[2],
                    lng2 = bounds[3], lat2 = bounds[4]) %>%
          addControl(title, position = "topright", className="map-title")
  
        ## save map as .png
        saveWidget(m, here("data/water/temp/temp.html"), selfcontained = TRUE)
        webshot(here("data/water/temp/temp.html"),
                file = paste0(here("data/water/zips//"),
                              sprintf("zip_%02s_", rasts_filter$zipcode[i]),
                              sprintf("%02s.png", rasts_filter$date[i])), 
                cliprect = "viewport",
                delay = 1,
                zoom = 4)
      }##END inner loop
   
   ## List all pics of zip code
   png_files <- list.files(here("data/water/zips//"),
                          pattern = zips_list[j],
                          full.names = TRUE)
   ## Output .mp4 from images
   av::av_encode_video(png_files, framerate = 2, 
                       output = paste0(here("data/water/vids//"),
                                       "zip_", zips_list[j], "_2022_2023.mp4"))
  # ## Output .gif from images
  # gif_convert <- function(x, output) {
  # image_read(x) %>% 
  #   image_animate(fps=1) %>% 
  #     ## try out "optimize" and fps arguments
  #     ## in this for future run
  #   image_write(output)
  # }
  # 
  # gif_convert(x= paste0(here("data/water/zips//"), png_files),
  #             output = paste0(here("data/water/gifs//"),
  #                             "zip_", zips_list[j], "_2022.gif"))
  
} ##END outer loop

```



## Temperature

The mean daily temperature by zip code was extracted from the PRISM dataset using a Google Earth Engine script. This was exported as a CSV with variables for date ("imageID"), zip code ("GEOID10"), and daily mean temperate ("mean"). We'll read in and clean up the CSV, as well as determine if each observation falls w/in the optimal range for WNV transmission. For *Culex tarsalis*, this is between 22.9-25.9C (optimum temp of 23.9 w/95% CI); for *Culex quinquefasciatus*, this is between 23.9-27.1C (optimum 25.2 w/95% CI). Becuse both species are present in Kern, we'll consider the optimal range from 22.9-27.1C (Shocket et al., 2020)
```{r}
temp <- read_csv(here('data/temp/kern_tmean_GEE_output.csv'))

temp2 <- temp %>% 
  ## extract date from PRISM image id
  mutate(date = lubridate::ymd(imageID)) %>% 
  dplyr::select(!imageID) %>% 
  ## rename GEE extract variables
  rename(tmean_c = mean,
         zipcode = GEOID10) %>% 
  ## find temp in F
  mutate(tmean_f = (tmean_c*(9/5))+32,
         .before = date) %>% 
  ## does a day fall w/in optimal temp range?
  mutate(cx_opt = case_when(tmean_c >= 12.1 & tmean_c < 22.9 ~ "in range",
                            tmean_c >= 22.9 & tmean_c <= 27.1 ~ "optimal",
                            tmean_c > 27.1 & tmean_c <=31.9 ~ "in range",
                            .default = "out range")) %>% 
  ## set T/F as factor for consistent graphic
  mutate(cx_opt = fct_relevel(cx_opt, levels = c("optimal", "in range", "out range")))

write_csv(temp2, here('data/temp/kern_tmean_20100401_20230930.csv'))
```

Some test plots
```{r}
temp3 <- temp2 %>% 
  filter(zipcode == 93252,
         date >= "2022-01-01" & date <= "2022-07-31")

test <- temp3 %>% 
  group_by(culex_range) %>% 
  summarize(count = sum(culex_range == "TRUE", na.rm = T))

ggplot(data = temp3, aes(x = date, y = tmean_f)) +
    geom_rect(xmin = -Inf, xmax = Inf, ymax = 78.6, ymin = 73.2,
            alpha = 0.01, fill = "gray75")+
        geom_point(size = 3, 
                   alpha = 0.7,
                   aes(color = cxTar_opt)) +
        scale_color_manual(name = "",
                           values = c("coral2", "dodgerblue"),
                           labels = c("", "In range"))+
        geom_line(linewidth = 0.7) +
        labs(y = "Mean daily temperature (F)",
             x = "Date") +
        scale_x_date(date_labels = "%b %y") +
        geom_hline(yintercept = 73.2, linetype = "dashed", color = "gray50")+
        geom_hline(yintercept = 78.6, linetype = "dashed", color = "gray50")+
  annotate("text", y = 74, x = as.Date(temp3$date[1]), 
           hjust = 0, vjust = 0,
           label = "Optimal range for\nWNV transmission",
           size = 3,
           fontface = "bold") +
        theme_classic() +
        theme(
          # axis.title.x = element_text(face = "bold", vjust = -1),
          axis.title.y = element_text(vjust = 2, size = 14),
          axis.title.x = element_text(vjust = -1, size = 14),
          axis.text = element_text(size = 13),
          legend.position = "none"
        )
```



## Trap data

Finally, we'll wrangle some trap data and aggregate it by zip code. Exact locations of traps should not be public, so instead we'll produce plots showing how the number of WNV positives found in traps across a zip code change by month/year.

Because trap data is aggregated and assigned to clusters, we'll first need to assign each cluster to a zip code. 
```{r}
clust <- read_sf(here("data/traps/andy/cluster_shp/clusterPolys.shp"))

zips <- read_sf(here("data/zipcodes/kern_zips.shp")) %>% 
  st_transform(crs = crs(clust))

clust_zips <- st_centroid(clust) %>% 
  st_join(zips) %>% 
  ## centroid of clust 7 and 95 *just* outside 93308
  ## manually adding to zips
  within(., zipcode[clust == 7] <- 93280) %>% 
  within(., area_m2[clust == 7] <- (zips$area_m2[zips$zipcode==93280])) %>% 
  within(., zipcode[clust == 95] <- 93308) %>% 
  within(., area_m2[clust == 95] <- zips$area_m2[zips$zipcode==93308])
```

Now we can assign zipcodes to abundance and MIR data:
```{r}
## WNV MIR
wnv <- read_csv(here("data/traps/andy/wnvMIRPIR1500LagWeeks0_NA.csv")) %>% 
  janitor::clean_names() %>% 
  ## assign zip by clust
  inner_join(x = .,y = clust_zips) %>% 
  ## filter for relevant vars
  dplyr::select(zipcode, clust:woy, date, 
                pool_size, num_pools, mir_all, mir_spline_all) %>%
  mutate(month = lubridate::month(date),
         .before = year)
write_csv(wnv, here("data/traps/plotting/wnvMIR_plotting.csv"))

## SLEV MIR
slev <- read_csv(here("data/traps/andy/slevMIRPIR1500LagWeeks0_NA.csv")) %>% 
  janitor::clean_names() %>% 
  inner_join(x=., y=clust_zips) %>% 
  dplyr::select(zipcode, clust:woy, date,
                pool_size, num_pools, mir_all, mir_spline_all) %>% 
  mutate(month = lubridate::month(date),
         .before = year)
write_csv(slev, here("data/traps/plotting/slevMIR_plotting.csv"))

### abundance
abund <- read_csv(here('data/traps/andy/all1500LagWeeks0_NA.csv')) %>% 
  janitor::clean_names() %>% 
  ## assign zip by clust
  inner_join(x=., y=clust_zips) %>% 
  ## filter for relevant vars
  dplyr::select(zipcode, clust:woy, collection_date, mos_per_trap_night) %>% 
  rename(date = collection_date) %>% 
  mutate(month = lubridate::month(date),
         .before = year)
write_csv(abund, here("data/traps/plotting/abundance_plotting.csv"))
```



TEST WRANGLING/PLOTS:
MIR:
```{r}
wnv <- read_csv(here("data/traps/wnvMIR_plotting.csv"))

zip = 93203

cases_zip_yr <- wnv %>% 
  filter(zipcode == zip, 
         date >= "2022-01-01" & date <= "2022-04-01") %>% 
  group_by(year, date) %>% 
  summarize(avgMIR = mean(mir_all, na.rm = TRUE))

zip_avg_cases <- wnv %>% 
  filter(zipcode == zip) %>% 
  group_by(woy) %>% 
  summarize(avg = mean(mir_all, na.rm = TRUE))

kern_avg_cases <- wnv %>% 
  group_by(woy) %>% 
  summarize(avg = mean(mir_all, na.rm = TRUE))

zip_avgMIR <- wnv %>%  
  filter(zipcode==zip)
zip_avgMIR <- mean(zip_avgMIR$mir_all, na.rm = TRUE)

kern_avgMIR <- mean(wnv$mir_all, na.rm = TRUE)

ggplot()+
  ## time frame in zip code
  geom_col(data = cases_zip_yr, 
             aes(x=date, y = avgMIR),
            fill = "sienna2", color = "sienna4") +
  # ## avg zipcode
  # geom_point(data = zip_avg_cases, 
  #            aes(x=woy, y = avg),
  #            color = "plum", size = 3, alpha = 0.6) +
  # geom_line(data = zip_avg_cases, 
  #            aes(x=woy, y = avg),
  #           color = "plum4") +
  # ## avg kern
  # geom_point(data = kern_avg_cases, 
  #            aes(x=woy, y = avg),
  #            color = "gray50", size = 3, alpha = 0.6) +
  # geom_line(data = kern_avg_cases, 
  #            aes(x=woy, y = avg),
  #           color = "black") +
  labs(y = "Average MIR",
       x = element_blank()) +
  theme_classic() +
  theme(
    # axis.title.x = element_text(face = "bold", vjust = -1),
    axis.title.y = element_text(face = 'bold', vjust = 3)
  )

```

Abundance:
```{r}
abund <- read_csv(here("data/traps/abundance_plotting.csv"))

zip = 93249

if (length(abund_cust$avg) == 0) {
  print("no data")
}

abund_cust <- abund %>% 
  filter(zipcode == zip, 
         date >= "2032-01-01" & date <= "2032-12-31") %>% 
  group_by(year, date) %>% 
  summarize(avg = mean(mos_per_trap_night, na.rm = TRUE))

zip_avg_abund <- abund %>% 
  filter(zipcode == zip) %>% 
  group_by(woy) %>% 
  summarize(avg = mean(mos_per_trap_night, na.rm = TRUE))

kern_avg_abund <- abund %>% 
  group_by(woy) %>% 
  summarize(avg = mean(mos_per_trap_night, na.rm = TRUE))

## avg values
cust_avgAbund <- abund %>% 
  filter(zipcode == zip,
         date >= "2022-01-01" & date <= "2022-12-31")
cust_avgAbund <- mean(cust_avgAbund$mos_per_trap_night, na.rm = TRUE)

zip_avgAbund <- abund %>%  
  filter(zipcode==zip)
zip_avgAbund <- mean(zip_avgAbund$mos_per_trap_night, na.rm = TRUE)

kern_time_avgAbund <- abund %>% 
  filter(date >= "2022-01-01" & date <= "2022-12-31")
kern_time_avgAbund <- mean(kern_time_avgAbund$mos_per_trap_night, na.rm = TRUE)


## abundance comp (filled)
ggplot()+
  ## avg kern
  geom_point(data = kern_avg_abund,
             aes(x=woy, y = avg),
             color = "gray30", size = 2.5, alpha = 0.6) +
  geom_line(data = kern_avg_abund,
             aes(x=woy, y = avg),
            color = "black", linewidth = 0.6) +
  geom_area(data = kern_avg_abund,
            aes(x=woy, y = avg),
            fill = "gray60", alpha=.4)+
   # avg zipcode
  geom_point(data = zip_avg_abund,
             aes(x=woy, y = avg),
             color = "plum", size = 2.5, alpha = 0.8) +
  geom_line(data = zip_avg_abund,
             aes(x=woy, y = avg),
            color = "purple4", linewidth = 0.7) +
   geom_area(data = zip_avg_abund,
             aes(x=woy, y = avg),
            fill = "plum", alpha=.5) +
  ## user defined zip and time
   geom_point(data = abund_cust, 
             aes(x=woy, y = avg),
             color = "sienna2", size = 2.5, alpha = 0.8) +
  geom_line(data = abund_cust, 
             aes(x=woy, y = avg),
            color = "sienna4", linewidth = 0.7) +
  geom_area(data = abund_cust, 
             aes(x=woy, y = avg),
            fill = "sienna2", alpha = .5) +
  labs(y = "Average weekly abundance",
       x = element_blank()) +
  theme_classic() +
  theme(
    # axis.title.x = element_text(face = "bold", vjust = -1),
    axis.title.y = element_text(face = 'bold', vjust = 3)
  )


## abundance comp (lines)
ggplot()+
  ## avg kern (in time period)
  geom_hline(yintercept = kern_time_avgAbund,
            color = "black", linetype = "dashed", linewidth = 0.8) +
  # annotate("text", y = (kern_time_avgAbund+0.07), x = abund_cust$date[1],
  #          hjust = 0,
  #          label = "Average abundance within time period",
  #          size = 3,
  #          fontface = "bold") +
   # avg zipcode
  geom_hline(yintercept = zip_avgAbund,
            color = "purple", linetype = "dashed", linewidth = 0.8) +
  # annotate("text", y = (zip_avgAbund+0.07), x = abund_cust$date[1],
  #          hjust = 0,
  #          label = "Average abundance across zip code",
  #          size = 3,
  #          color = "purple4",
  #          fontface = "bold") +
  ## user defined zip/year
   geom_point(data = abund_cust, 
             aes(x=date, y = avg),
             color = "sienna2", size = 3, alpha = 0.7) +
  geom_line(data = abund_cust, 
             aes(x=date, y = avg),
            color = "sienna4", linewidth = 0.8) +
  # geom_hline(yintercept = cust_avgAbund,
  #            color = "sienna", linetype= "dashed", linewidth = 0.8)+
  # annotate("text", y = (cust_avgAbund+0.07), x = abund_cust$date[1],
  #          hjust = 0,
  #          label = "Average abundance for zipcode and time period",
  #          size = 3,
  #          color = "sienna4",
  #          fontface = "bold") +
  # geom_area(data = abund_cust, 
  #            aes(x=woy, y = avg),
  #           fill = "sienna2", alpha = .5) +
  labs(y = "Average weekly abundance",
       x = element_blank()) +
  scale_x_date(date_labels = "%b %y",
               date_breaks = "1 month") +
  theme_classic() +
  theme(
    # axis.title.x = element_text(face = "bold", vjust = -1),
    axis.title.y = element_text(face = 'bold', vjust = 3),
    legend.position = "right"
  )
```


## Transmission efficiency

Here we'll find an average WNV transmission efficiency for each zipcode.
*NOTE:* This may be replaced by R0 once the model is done.
```{r}
## Read in data
wnv_trans <- rast(here('data/Kern_transmission_raster_wgs84.tif'))

## easier to transform vector than high-res raster
kern_zips <- vect(here("data/zipcodes/kern_zips.shp")) %>% 
  project(y = crs(wnv_trans))

## find total average
trans_kern <- global(wnv_trans, "mean", na.rm = T)
trans_kern <- data.frame("trans_eff" = trans_kern[1,1], "zipcode" = "Kern")

## find the mean value per zip
trans_zonal <- terra::zonal(wnv_trans, kern_zips, fun = 'mean', na.rm = T)

trans_zonal_zips <- trans_zonal %>% 
  mutate(zipcode = kern_zips$zipcode) %>% 
  rename(trans_eff = Kern_transmission_raster_wgs84) %>% 
  rbind(., trans_kern)

write_csv(trans_zonal_zips, here('data/transmission_efficiency_zipcodes.csv'))
```
